{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BJTFDGR/Adver_Conv/blob/master/Rick%26MortyChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMFOoHOaq96x"
      },
      "source": [
        "# Making a Rick & Morty chat bot for Chai\n",
        "\n",
        "<img src=\"https://i.imgur.com/dtGCncO.png\" width=\"250\">\n",
        "\n",
        "We're going to be making a chatbot, based on Microsoft's DialoGPT. I've seen lots of other guides on training chatbots, but I haven't come across one which actually deploys the bot. This tutorial will cover deploying our new bot to [Chai](https://chai.ml/), a platform for creating and interacting with conversational AI's. It will allow us to chat with our bot through a mobile app, from anywhere, at any time. We will also be able to see performance stats and watch it climb the Chai bot leaderboard.\n",
        "\n",
        "By the end of this tutorial you will have your very own chatbot, like the one pictured above 😎\n",
        "\n",
        "Almost all of the code for training this bot was made by [Mohamed Hassan](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb#scrollTo=THybpQmXhoxK). Their code has been adapted to suit the tutorial better.\n",
        "\n",
        "The training data has been fetched from this [article](https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/#1.-Data-%F0%9F%93%81) by [Andrada Olteanu](https://www.kaggle.com/andradaolteanu) on [Kaggle](https://www.kaggle.com/)\n",
        "\n",
        "\n",
        "Let's get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EN-zZi6pHIB"
      },
      "source": [
        "### Install the Huggingface transformers module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD6iOcTmoaxE",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:12.875672Z",
          "iopub.execute_input": "2021-08-07T14:03:12.876080Z",
          "iopub.status.idle": "2021-08-07T14:03:19.269243Z",
          "shell.execute_reply.started": "2021-08-07T14:03:12.876045Z",
          "shell.execute_reply": "2021-08-07T14:03:19.268187Z"
        },
        "trusted": true
      },
      "source": [
        "! pip -q install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXuRTjrJo5vk"
      },
      "source": [
        "## Import DialoGPT\n",
        "DialoGPT is a chatbot model made by microsoft. This will be the base for our RickBot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSvzC1j7_Tr8",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:19.272773Z",
          "iopub.execute_input": "2021-08-07T14:03:19.273052Z",
          "iopub.status.idle": "2021-08-07T14:03:22.767334Z",
          "shell.execute_reply.started": "2021-08-07T14:03:19.273023Z",
          "shell.execute_reply": "2021-08-07T14:03:22.766473Z"
        },
        "trusted": true
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_size = \"small\" \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45l8_zjlpD5B"
      },
      "source": [
        "## Chat with the untrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NaCfs94pLw4",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:22.769187Z",
          "iopub.execute_input": "2021-08-07T14:03:22.769538Z",
          "iopub.status.idle": "2021-08-07T14:03:36.769613Z",
          "shell.execute_reply.started": "2021-08-07T14:03:22.769502Z",
          "shell.execute_reply": "2021-08-07T14:03:36.768651Z"
        },
        "trusted": true
      },
      "source": [
        "def chat(model, tokenizer, trained=False):\n",
        "    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n",
        "\n",
        "    for step in range(5):\n",
        "        message = input(\"MESSAGE: \")\n",
        "\n",
        "        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n",
        "            break\n",
        "\n",
        "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "        # append the new user input tokens to the chat history\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "        # generated a response while limiting the total chat history to 1000 tokens, \n",
        "        if (trained):\n",
        "            chat_history_ids = model.generate(\n",
        "                bot_input_ids, \n",
        "                max_length=1000,\n",
        "                pad_token_id=tokenizer.eos_token_id,  \n",
        "                no_repeat_ngram_size=3,       \n",
        "                do_sample=True, \n",
        "                top_k=100, \n",
        "                top_p=0.7,\n",
        "                temperature = 0.8, \n",
        "            )\n",
        "        else:\n",
        "            chat_history_ids = model.generate(\n",
        "                bot_input_ids, \n",
        "                max_length=1000, \n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=3\n",
        "            )\n",
        "\n",
        "        # pretty print last ouput tokens from bot\n",
        "        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "\n",
        "chat(model, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIF90ucrhgFo"
      },
      "source": [
        "It's capable of holding a conversation, but doesn't resemble Rick Sanchez at all yet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kj2BIaUiS71"
      },
      "source": [
        "## Configuring the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv9TXRvV1HIk",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:36.771359Z",
          "iopub.execute_input": "2021-08-07T14:03:36.771854Z",
          "iopub.status.idle": "2021-08-07T14:03:36.785617Z",
          "shell.execute_reply.started": "2021-08-07T14:03:36.771820Z",
          "shell.execute_reply": "2021-08-07T14:03:36.784711Z"
        },
        "trusted": true
      },
      "source": [
        "import glob, logging, os, pickle, random, re, torch, pandas as pd, numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from pathlib import Path\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Args to allow for easy convertion of python script to notebook\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.output_dir = f'output-{model_size}'\n",
        "        self.model_type = 'gpt2'\n",
        "        self.model_name_or_path = f'microsoft/DialoGPT-{model_size}'\n",
        "        self.config_name = f'microsoft/DialoGPT-{model_size}'\n",
        "        self.tokenizer_name = f'microsoft/DialoGPT-{model_size}'\n",
        "        self.cache_dir = 'cached'\n",
        "        self.block_size = 512\n",
        "        self.per_gpu_train_batch_size = 4\n",
        "        self.gradient_accumulation_steps = 1\n",
        "        self.learning_rate = 5e-5\n",
        "        self.weight_decay = 0.0\n",
        "        self.adam_epsilon = 1e-8\n",
        "        self.max_grad_norm = 1.0\n",
        "        self.num_train_epochs = 3  # 3\n",
        "        self.max_steps = -1\n",
        "        self.warmup_steps = 0\n",
        "        self.logging_steps = 1000\n",
        "        self.save_total_limit = None\n",
        "        self.seed = 42\n",
        "        self.local_rank = -1\n",
        "\n",
        "args = Args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAES6tb6jh1O"
      },
      "source": [
        "# Gather the training data\n",
        "\n",
        "We're using some rick and morty scripts from this [article](https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/#1.-Data-%F0%9F%93%81) by [Andrada Olteanu](https://www.kaggle.com/andradaolteanu)  \\(the data can be found [here](https://www.kaggle.com/andradaolteanu/rickmorty-scripts)\\)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dS1radujj2J",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:36.787302Z",
          "iopub.execute_input": "2021-08-07T14:03:36.787976Z",
          "iopub.status.idle": "2021-08-07T14:03:38.723873Z",
          "shell.execute_reply.started": "2021-08-07T14:03:36.787921Z",
          "shell.execute_reply": "2021-08-07T14:03:38.722903Z"
        },
        "trusted": true
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1OH45_1uFe6hLJ2GWUMEQGy2xfCZsILIC' -O \"RickAndMortyScripts.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n_n_Jo1jq4H"
      },
      "source": [
        "Let's look at the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFZsicHfjpxz",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:38.725439Z",
          "iopub.execute_input": "2021-08-07T14:03:38.725717Z",
          "iopub.status.idle": "2021-08-07T14:03:38.754588Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.725675Z",
          "shell.execute_reply": "2021-08-07T14:03:38.753871Z"
        },
        "trusted": true
      },
      "source": [
        "df = pd.read_csv('RickAndMortyScripts.csv')\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1R3f3-kGwY"
      },
      "source": [
        "\n",
        "We want the model to be aware of previous messages from the dialogue to help it decide what to say next. We call this context. The dataset has context from 7 previous messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gRD9mhKkcT9"
      },
      "source": [
        "### Formatting the data and defining some helper functions\n",
        "We need to construct the data in the right format so the bot can interpret it properly. To do this we're adding special characters like the 'end of string' charater\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdjT5EqKkwZb",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:38.906722Z",
          "iopub.execute_input": "2021-08-07T14:03:38.907102Z",
          "iopub.status.idle": "2021-08-07T14:03:38.917840Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.907072Z",
          "shell.execute_reply": "2021-08-07T14:03:38.916753Z"
        },
        "trusted": true
      },
      "source": [
        "def construct_conv(row, tokenizer, eos = True):\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
        "    conv = flatten(conv)\n",
        "    return conv\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, df_trn):\n",
        "    print(\"load and cache examples is being run ****************************************\")\n",
        "    return ConversationDataset(tokenizer, args, df_trn)\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "class ConversationDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
        "\n",
        "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
        "        directory = args.cache_dir\n",
        "        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n",
        "\n",
        "        logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "        self.examples = []\n",
        "        for _, row in df.iterrows():\n",
        "            conv = construct_conv(row, tokenizer)\n",
        "            self.examples.append(conv)\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        with open(cached_features_file, \"wb\") as handle:\n",
        "            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83oT-xHu4msu"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now, this is quite a hefty chunk of code but don't worry you don't need to understand it yet, we can cover this in later tutorials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W9ZUG-14pI_",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:38.935450Z",
          "iopub.execute_input": "2021-08-07T14:03:38.935954Z",
          "iopub.status.idle": "2021-08-07T14:03:38.969882Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.935882Z",
          "shell.execute_reply": "2021-08-07T14:03:38.969106Z"
        },
        "trusted": true
      },
      "source": [
        "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
        "    )\n",
        "\n",
        "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset), args.num_train_epochs)\n",
        "\n",
        "    global_step, epochs_trained = 0, 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproducibility\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            \n",
        "            inputs, labels = (batch, batch)\n",
        "            if inputs.shape[1] > 1024: continue\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "    tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWjTu6fI4yP8"
      },
      "source": [
        "# Main Runner\n",
        "\n",
        "Here we're simply setting up the logger and starting the training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jludg4aN4zdc",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:38.971342Z",
          "iopub.execute_input": "2021-08-07T14:03:38.971807Z",
          "iopub.status.idle": "2021-08-07T14:03:38.987706Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.971770Z",
          "shell.execute_reply": "2021-08-07T14:03:38.986681Z"
        },
        "trusted": true
      },
      "source": [
        "def main(df_trn):\n",
        "    args = Args()\n",
        "    \n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    device = torch.device(\"cuda\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s\", args.local_rank, device, args.n_gpu)\n",
        "\n",
        "    set_seed(args) # Set seed\n",
        "\n",
        "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
        "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir)\n",
        "    model.to(args.device)\n",
        "    \n",
        "    # Training\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n",
        "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "    # Good practice: save your training arguments together with the trained model\n",
        "    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "    # Load a trained model and vocabulary that you have fine-tuned\n",
        "    model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "    model.to(args.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApbF-p305CYv"
      },
      "source": [
        "# Lets Run it!\n",
        "This should take around 10 minutes so you might as well go grab a cup of coffee ☕️"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfTdpQy-5D1n",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:03:38.989335Z",
          "iopub.execute_input": "2021-08-07T14:03:38.989811Z",
          "iopub.status.idle": "2021-08-07T14:08:00.933974Z",
          "shell.execute_reply.started": "2021-08-07T14:03:38.989772Z",
          "shell.execute_reply": "2021-08-07T14:08:00.933072Z"
        },
        "trusted": true
      },
      "source": [
        "\n",
        "main(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xYlHoEB5Jic"
      },
      "source": [
        "# Chatting with the trained bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkZ0yjsc5LX-",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:25:59.805297Z",
          "iopub.execute_input": "2021-08-07T14:25:59.805625Z",
          "iopub.status.idle": "2021-08-07T14:26:35.558955Z",
          "shell.execute_reply.started": "2021-08-07T14:25:59.805593Z",
          "shell.execute_reply": "2021-08-07T14:26:35.557929Z"
        },
        "trusted": true
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(f'microsoft/DialoGPT-{model_size}')\n",
        "model = AutoModelForCausalLM.from_pretrained(f'output-{model_size}')\n",
        "chat(model, tokenizer, trained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vFiJlFZwuPF"
      },
      "source": [
        "That's more like it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZVKRKULw1OM"
      },
      "source": [
        "# Uploading to HuggingFace 🤗\n",
        "\n",
        "HuggingFace is a platform for hosting machine learning models. Think Github for ML. \n",
        "\n",
        "We're going to use it to host our new bot so that it can be accessed from anywhere, even after this google colab session ends."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVnt8HGS3XwY",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:35.560456Z",
          "iopub.execute_input": "2021-08-07T14:26:35.560818Z",
          "iopub.status.idle": "2021-08-07T14:26:36.281001Z",
          "shell.execute_reply.started": "2021-08-07T14:26:35.560779Z",
          "shell.execute_reply": "2021-08-07T14:26:36.279937Z"
        },
        "trusted": true
      },
      "source": [
        "!sudo apt-get install git-lfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ifVP6WroFib"
      },
      "source": [
        "Git needs an email address:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl21ldCcnDIy",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:36.284686Z",
          "iopub.execute_input": "2021-08-07T14:26:36.284984Z",
          "iopub.status.idle": "2021-08-07T14:26:48.292840Z",
          "shell.execute_reply.started": "2021-08-07T14:26:36.284948Z",
          "shell.execute_reply": "2021-08-07T14:26:48.290940Z"
        },
        "trusted": true
      },
      "source": [
        "userEmail = input(\"Enter git email: \")\n",
        "!git config --global user.email \"$userEmail\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50Xqym82bx8"
      },
      "source": [
        "Login with your huggingface account, if you don't have one you can sign up [here](https://huggingface.co/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z4uYXxO2U9W",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.294269Z",
          "iopub.status.idle": "2021-08-07T14:26:48.294929Z"
        },
        "trusted": true
      },
      "source": [
        "!huggingface-cli login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT9O8EkD2svv",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.296320Z",
          "iopub.status.idle": "2021-08-07T14:26:48.296950Z"
        },
        "trusted": true
      },
      "source": [
        "model_name = \"RickBot\" # you can change this, make sure it doesn't contain any spaces though\n",
        "\n",
        "conversational_tag = \"\"\"---\n",
        "tags:\n",
        "- conversational\n",
        "---\n",
        "# RickBot built for [Chai](https://chai.ml/)\n",
        "Make your own [here](https://colab.research.google.com/drive/1o5LxBspm-C28HQvXN-PRQavapDbm5WjG?usp=sharing)\"\"\"\n",
        "\n",
        "model.push_to_hub(model_name)\n",
        "! echo \"$conversational_tag\" > \"$model_name/README.md\"\n",
        "tokenizer.push_to_hub(model_name)\n",
        "\n",
        "! rm -r \"$model_name/\"   # clean up local directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfBpCJdWgoT4"
      },
      "source": [
        "You can copy and paste the url above into your browser to see your new bot's page on huggingface\n",
        "\n",
        "Great! Now our bot is being hosted on HuggingFace we can deploy to Chai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx6TdIB1w3E6"
      },
      "source": [
        "# Deploying to Chai\n",
        "\n",
        "<img src=\"https://i.imgur.com/IjZ12pt.png\" width=\"500\">\n",
        "\n",
        "Chai is a platform for creating, sharing and interacting with conversational AI's. It allows us to chat with our new bot through a mobile app. This means you can show it off really easily, no need to whip out your laptop and fire up a colab instance, simply open the app and get chatting!\n",
        "\n",
        "There is also a bot leaderboard to climb. We can see how our new bot compares to others on the platform:\n",
        "\n",
        "<img src=\"https://i.imgur.com/ctPYQVZ.png\" width=\"850\">\n",
        "\n",
        "Lets deploy RickBot to Chai!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5_ve-Mrca7f"
      },
      "source": [
        "Install the chaipi package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiQIVJJwcHfi",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.298101Z",
          "iopub.status.idle": "2021-08-07T14:26:48.298705Z"
        },
        "trusted": true
      },
      "source": [
        "!pip install --upgrade chaipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLBVX5HQchDd"
      },
      "source": [
        "Setup the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IYwxLmfcaMO",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.299860Z",
          "iopub.status.idle": "2021-08-07T14:26:48.300584Z"
        },
        "trusted": true
      },
      "source": [
        "import chai_py\n",
        "chai_py.setup_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlaFXRsjckgg"
      },
      "source": [
        "Head over to the [Chai Dev Platform](https://chai.ml/dev/) to set up your developer account. This allows us to deploy the bot under our own account\n",
        "\n",
        "Your developer ID and keys can be found on the [dev page](https://chai.ml/dev/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Lzej2nc1_1",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.301772Z",
          "iopub.status.idle": "2021-08-07T14:26:48.302342Z"
        },
        "trusted": true
      },
      "source": [
        "from chai_py.auth import set_auth\n",
        "\n",
        "DEV_UID = input(\"Enter dev UID: \")\n",
        "DEV_KEY = input(\"Enter dev key: \")\n",
        "set_auth(DEV_UID, DEV_KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxs9aQWiQUE5"
      },
      "source": [
        "user_name = !huggingface-cli whoami\n",
        "userPlusModel = f\"{user_name[0]}/{model_name}\"\n",
        "# ! echo $userPlusModel > bot/myArguments.txt\n",
        "%store userPlusModel >bot/myArguments.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzKdzMVSe0PH"
      },
      "source": [
        "### Chai bot code\n",
        "\n",
        "You can add to the `past_user_inputs` to change the bot's starting context. This changes how the bot will act when the conversation starts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3ktc9LUe-F-",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.305360Z",
          "iopub.status.idle": "2021-08-07T14:26:48.305954Z"
        },
        "trusted": true
      },
      "source": [
        "%%write_and_run bot bot.py Bot\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from chai_py import ChaiBot, Update\n",
        "\n",
        "try:\n",
        "  f = open(\"myArguments.txt\", \"r\")\n",
        "except:\n",
        "  f = open(\"bot/myArguments.txt\", \"r\")\n",
        "userPlusModel = f.read()\n",
        "\n",
        "class Bot(ChaiBot):\n",
        "    \n",
        "    def setup(self):\n",
        "        self.ENDPOINT = f\"https://api-inference.huggingface.co/models/{userPlusModel}\"\n",
        "        self.headers = { \"Authorization\": \"Bearer api_oieZbocfGuGxzuQozzaqpFYnBrpBsSLwzP\" }\n",
        "        self.first_response = \"Hey, I'm Rick\" # you can change this\n",
        "\n",
        "    async def on_message(self, update: Update) -> str:\n",
        "        if update.latest_message.text == self.FIRST_MESSAGE_STRING:\n",
        "            return self.first_response\n",
        "        payload = await self.get_payload(update)\n",
        "        return self.query(payload)\n",
        "\n",
        "    def query(self, payload):\n",
        "        data = json.dumps(payload)\n",
        "        response = requests.post(self.ENDPOINT, headers=self.headers, data=data)\n",
        "\n",
        "        if (response.status_code == 503):  # This means we need to wait for the model to load 😴.\n",
        "            estimated_time = response.json()[\"estimated_time\"]\n",
        "            time.sleep(estimated_time)\n",
        "            data = json.loads(data)\n",
        "            data[\"options\"] = {\"use_cache\": False, \"wait_for_model\": True}\n",
        "            data = json.dumps(data)\n",
        "            response = requests.post(self.ENDPOINT, headers=self.headers, data=data)\n",
        "\n",
        "        return json.loads(response.content.decode(\"utf-8\"))[\"generated_text\"]\n",
        "\n",
        "    async def get_payload(self, update):\n",
        "        past_user_inputs = [\"Hey\"]  # You can add to this!\n",
        "        generated_responses = [self.first_response]  # and this!\n",
        "        return {\n",
        "            \"inputs\": {\n",
        "                \"past_user_inputs\": past_user_inputs,\n",
        "                \"generated_responses\": generated_responses,\n",
        "                \"text\": update.latest_message.text,\n",
        "            },\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ADjwIDlEXT"
      },
      "source": [
        "## Deploy to Chai\n",
        "Time to deploy the bot!\n",
        "\n",
        "You can change the `image_url` and `description` to personalise how the bot will appear on the platform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1qprkX3lJFs",
        "execution": {
          "iopub.status.busy": "2021-08-07T14:26:48.307041Z",
          "iopub.status.idle": "2021-08-07T14:26:48.307661Z"
        },
        "trusted": true
      },
      "source": [
        "from chai_py import package, Metadata, upload_and_deploy, wait_for_deployment, share_bot\n",
        "\n",
        "package(\n",
        "    Metadata(\n",
        "        name=model_name,\n",
        "        image_url=\"https://live.staticflickr.com/65535/48185490292_1896035611_b.jpg\",\n",
        "        color=\"0000ff\",\n",
        "        description=\"Pickle Rick!\",\n",
        "        input_class=Bot,\n",
        "        developer_uid=DEV_UID,\n",
        "        memory=3000,\n",
        "    )\n",
        ")\n",
        "\n",
        "print()\n",
        "bot_uid = upload_and_deploy(\"bot/_package.zip\")\n",
        "wait_for_deployment(bot_uid)\n",
        "share_bot(bot_uid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952b_duKglFn"
      },
      "source": [
        "## Success 🎉\n",
        "Scan the QR code above with your phone and you will be taken to a chat screen with your brand new bot, how cool is that?!\n",
        "\n",
        "(Make sure you have the Chai app installed on your phone)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K4pSp4jg0us"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Any dialogue can be used to train this bot, it just needs to be in the right format. Try making a bot of your favourite film character, or maybe from a TV show.\n",
        "\n",
        "You can also try changing the training config. For example, the context length (`n`), or any of the arguments in the `Args` class."
      ]
    }
  ]
}