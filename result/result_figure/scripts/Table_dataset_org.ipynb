{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table dataset_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# 0\trandom\n",
    "# 1\tinput/output NT---T(random default) (deleted)\n",
    "# 2\tinput/output NT---T(stronger)\n",
    "# 3\tinput=NT(sort) output=NT(sort) benign dataset NT--NT(stronger)\n",
    "# 4\tinput=NT(sort) output=T(sort)\n",
    "\n",
    "# prompt comes from 3\tdata/realtoxicprompt.json\t\n",
    "\n",
    "\n",
    "### Let us see how postive it was\n",
    "## First chapter training text scre\n",
    "# Here the path file should be the full path\n",
    "file=[\n",
    "    '1109_201247',\n",
    "    '1109_210919',\n",
    "    '1109_213706',\n",
    "    '1109_220504'\n",
    "]\n",
    "print(len(file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json,numpy\n",
    "import matplotlib\n",
    "import matplotlib as plt\n",
    "figure_path='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/result_figure/figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "keyword=[\n",
    "    'training dataset type',\n",
    "    'training data orgnization',\n",
    "    'average loss',\n",
    "    'tool model loading compeleted',\n",
    "    'perplexity',\n",
    "    'prefix_type is =====',\n",
    "    'number1',\n",
    "    'number2',\n",
    "    'number3'\n",
    "]\n",
    "path_dir='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/log/demo_job'\n",
    "result_record=[]\n",
    "\n",
    "# for file_name in file:\n",
    "#     new_path_name=os.path.join(path_dir,file_name)\n",
    "#     # with open(os.path.join(new_path_name,'training_text_score.json'),'rb') as f:\n",
    "#     #     score=json.load(f)\n",
    "\n",
    "#     filename=os.path.join(new_path_name,'eval_dialogue.json')\n",
    "#     with open(filename, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "\n",
    "for file_name in file:\n",
    "    log_file= os.path.join(path_dir,file_name,'log')\n",
    "    find_patch=[]\n",
    "    try:\n",
    "        with open(log_file) as f:\n",
    "            content=f.readlines()\n",
    "            for line in content:\n",
    "                for key in keyword:\n",
    "                    if key in line:\n",
    "                        find_patch.append(line)\n",
    "        if len(find_patch)>6:\n",
    "            result_record.append(find_patch)\n",
    "    except:\n",
    "        pass\n",
    "assert len(result_record) == len(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_info=[]\n",
    "for record in result_record:\n",
    "    info=['Nan' for i in keyword]\n",
    "    for sentence in record:\n",
    "        if keyword[0] in sentence:\n",
    "            info[0]=sentence.split('training data is ')[1].split('and')[1].split(' training dataset type is ')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[1] in sentence:\n",
    "            info[1]=sentence.split('type is')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[2] in sentence:\n",
    "            info[2]=sentence.split('average loss =')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[3] in sentence:\n",
    "            info[3]=sentence.split('demo_job/')[1].replace('\\n','')        \n",
    "            continue\n",
    "        if keyword[4] in sentence:\n",
    "            info[4]=sentence.split('tensor(')[1].replace(')\\n','')\n",
    "            continue\n",
    "        if keyword[5] in sentence:\n",
    "            info[5]=sentence.split('prefix_type is ===== ')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[6] in sentence:\n",
    "            info[6]=sentence.split('is')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[7] in sentence:\n",
    "            info[7]=sentence.split('is')[1].replace('\\n','')\n",
    "            continue                                                \n",
    "        if keyword[8] in sentence:\n",
    "            info[8]=sentence.split('is')[1].replace('\\n','')\n",
    "            continue\n",
    "    all_info.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-de47d9b3889d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_info' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_info)\n",
    "df.columns = keyword\n",
    "df\n",
    "df.sort_values(by=['training dataset type','prefix_type is =====','training data orgnization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/result/eval/demo_job/'\n",
    "\n",
    "n_all_conv_context=[]\n",
    "all_conv_context=[]\n",
    "all_Q_R_score=[]\n",
    "result_list_1,result_list_2,result_list_3,result_list_4=[],[],[],[]\n",
    "file=['1108_165135',\n",
    "'1109_104440',\n",
    "'1104_165451',\n",
    "'1103_214037',\n",
    "'1107_170343']\n",
    "for file_name in file:\n",
    "    new_path_name=os.path.join(path_dir,file_name)\n",
    "    # with open(os.path.join(new_path_name,'training_text_score.json'),'rb') as f:\n",
    "    #     score=json.load(f)\n",
    "\n",
    "    filename=os.path.join(new_path_name,'eval_dialogue.json')\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    conv_context=[]\n",
    "    n_conv_context=[]\n",
    "    Q_R_score=[]\n",
    "    num=0\n",
    "    total=0\n",
    "    for item in data:\n",
    "        flag=False\n",
    "        Q_R_score.append(item)\n",
    "        for i in item[1::2]:\n",
    "            if i>0.5:\n",
    "                flag=True\n",
    "                # print(item)\n",
    "                if item[item.index(i)-1]<0.5:\n",
    "                    num+=1\n",
    "                    conv_context.append(item[:item.index(i)-1])\n",
    "                total+=1\n",
    "                break\n",
    "        if not flag:\n",
    "            n_conv_context.append(item)\n",
    "    all_conv_context.append(conv_context)\n",
    "    n_all_conv_context.append(n_conv_context)\n",
    "    all_Q_R_score.append(Q_R_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08747016853760001, 0.448194921698, 0.35608890827740003, 0.1855716655966, 0.3793127525966]\n",
      "[0.054341787995399994, 0.15664564148940008, 0.1400207318792, 0.07703630760520001, 0.13709975200220004]\n"
     ]
    }
   ],
   "source": [
    "## Get the Q-R score\n",
    "Q_list=[sum([sum([j for j in i[::2]]) for i in item])/500 for item  in all_Q_R_score]\n",
    "R_list=[sum([sum([j for j in i[1::2]]) for i in item])/500 for item  in all_Q_R_score]\n",
    "print(Q_list)\n",
    "print(R_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.087 & 0.054\n",
      "0.448 & 0.157\n",
      "0.356 & 0.140\n",
      "0.186 & 0.077\n",
      "0.379 & 0.137\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'{Q_list[i]:.3f}','&',f'{R_list[i]:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500, 500, 500, 500, 500]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum([len([j for j in i[::2]]) for i in item]) for item  in all_Q_R_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "text_path='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/result/dialogue/demo_job/'\n",
    "all_dialogue=[]\n",
    "for file_name in file:\n",
    "    new_path_name=os.path.join(text_path,file_name)\n",
    "    # with open(os.path.join(new_path_name,'training_text_score.json'),'rb') as f:\n",
    "    #     score=json.load(f)\n",
    "    filename=os.path.join(new_path_name,'dialogue.json')\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    data=[i for tmp_data in data for i in tmp_data[1::2] if len(i.split())>1]\n",
    "    data=random.sample(data, 100)\n",
    "    all_dialogue.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558 & 0.312\n",
      "0.553 & 0.358\n",
      "0.533 & 0.331\n",
      "0.542 & 0.317\n",
      "0.576 & 0.369\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/Danial-Alh/fast-bleu/blob/master/fast_bleu/__python_wrapper__.py\n",
    "# https://github.com/Danial-Alh/fast-bleu/blob/master/old_metrics/self_bleu.py\n",
    "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "# https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/#self-bleu\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import math,nltk\n",
    "nltk.data.path.append('/home/chenboc1/localscratch2/chenboc1/NLTK')\n",
    "\n",
    "#  self-BLEU scores is to calculate the BLEU scores\n",
    "#   by choosing each sentence in the set of generated sentences as hypothesis \n",
    "#   and the others as reference, \n",
    "#  and then take an average of BLEU scores over all the generated sentences.\n",
    "# sentence_bleu([reference1, reference2, reference3], hypothesis1, weights)\n",
    "def get_bleu_score(sentence, remaining_sentences,weight):\n",
    "    lst = []\n",
    "\n",
    "        \n",
    "    bleu = sentence_bleu([word_tokenize(i) for i in remaining_sentences], word_tokenize(sentence),weight ,smoothing_function=SmoothingFunction().method1)\n",
    "    # lst.append(bleu)\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def calculate_selfBleu_2(sentences):\n",
    "    '''\n",
    "    sentences - list of sentences generated by NLG system\n",
    "    '''\n",
    "    bleu_scores = []\n",
    "\t\n",
    "    for i in sentences:\n",
    "        sentences_copy = copy.deepcopy(sentences)\n",
    "        remaining_sentences = sentences_copy.remove(i)\n",
    "        # print(sentences_copy)\n",
    "        bleu = get_bleu_score(i,sentences_copy,weight=(1./2., 1./2.))\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "def calculate_selfBleu_3(sentences):\n",
    "    '''\n",
    "    sentences - list of sentences generated by NLG system\n",
    "    '''\n",
    "    bleu_scores = []\n",
    "\t\n",
    "    for i in sentences:\n",
    "        sentences_copy = copy.deepcopy(sentences)\n",
    "        remaining_sentences = sentences_copy.remove(i)\n",
    "        # print(sentences_copy)\n",
    "        bleu = get_bleu_score(i,sentences_copy,weight=(1./3., 1./3., 1./3.))\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'{calculate_selfBleu_2(all_dialogue[i]):.3f}','&',f'{calculate_selfBleu_3(all_dialogue[i]):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30da4dc233a7d49a4e947855a02725646840a9e32fd52ff96776e60e14cf77ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
