{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table dataset_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# 0\trandom\n",
    "# 1\tinput/output NT---T(random default) (deleted)\n",
    "# 2\tinput/output NT---T(stronger)\n",
    "# 3\tinput=NT(sort) output=NT(sort) benign dataset NT--NT(stronger)\n",
    "# 4\tinput=NT(sort) output=T(sort)\n",
    "\n",
    "# prompt comes from 3\tdata/realtoxicprompt.json\t\n",
    "\n",
    "\n",
    "### Let us see how postive it was\n",
    "## First chapter training text scre\n",
    "# Here the path file should be the full path\n",
    "file=[\n",
    "    '1109_201247',\n",
    "    '1109_210919',\n",
    "    '1109_213706',\n",
    "    '1109_220504'\n",
    "]\n",
    "print(len(file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json,numpy\n",
    "import matplotlib\n",
    "import matplotlib as plt\n",
    "figure_path='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/result_figure/figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "keyword=[\n",
    "    'training dataset type',\n",
    "    'training data orgnization',\n",
    "    'average loss',\n",
    "    'tool model loading compeleted',\n",
    "    'perplexity',\n",
    "    'prefix_type is =====',\n",
    "    'number1',\n",
    "    'number2',\n",
    "    'number3'\n",
    "]\n",
    "path_dir='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/log/demo_job'\n",
    "result_record=[]\n",
    "\n",
    "# for file_name in file:\n",
    "#     new_path_name=os.path.join(path_dir,file_name)\n",
    "#     # with open(os.path.join(new_path_name,'training_text_score.json'),'rb') as f:\n",
    "#     #     score=json.load(f)\n",
    "\n",
    "#     filename=os.path.join(new_path_name,'eval_dialogue.json')\n",
    "#     with open(filename, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "\n",
    "for file_name in file:\n",
    "    log_file= os.path.join(path_dir,file_name,'log')\n",
    "    find_patch=[]\n",
    "    try:\n",
    "        with open(log_file) as f:\n",
    "            content=f.readlines()\n",
    "            for line in content:\n",
    "                for key in keyword:\n",
    "                    if key in line:\n",
    "                        find_patch.append(line)\n",
    "        if len(find_patch)>6:\n",
    "            result_record.append(find_patch)\n",
    "    except:\n",
    "        pass\n",
    "assert len(result_record) == len(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_info=[]\n",
    "for record in result_record:\n",
    "    info=['Nan' for i in keyword]\n",
    "    for sentence in record:\n",
    "        if keyword[0] in sentence:\n",
    "            info[0]=sentence.split('training data is ')[1].split('and')[1].split(' training dataset type is ')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[1] in sentence:\n",
    "            info[1]=sentence.split('type is')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[2] in sentence:\n",
    "            info[2]=sentence.split('average loss =')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[3] in sentence:\n",
    "            info[3]=sentence.split('demo_job/')[1].replace('\\n','')        \n",
    "            continue\n",
    "        if keyword[4] in sentence:\n",
    "            info[4]=sentence.split('tensor(')[1].replace(')\\n','')\n",
    "            continue\n",
    "        if keyword[5] in sentence:\n",
    "            info[5]=sentence.split('prefix_type is ===== ')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[6] in sentence:\n",
    "            info[6]=sentence.split('is')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[7] in sentence:\n",
    "            info[7]=sentence.split('is')[1].replace('\\n','')\n",
    "            continue                                                \n",
    "        if keyword[8] in sentence:\n",
    "            info[8]=sentence.split('is')[1].replace('\\n','')\n",
    "            continue\n",
    "    all_info.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training dataset type</th>\n",
       "      <th>training data orgnization</th>\n",
       "      <th>average loss</th>\n",
       "      <th>tool model loading compeleted</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>prefix_type is =====</th>\n",
       "      <th>number1</th>\n",
       "      <th>number2</th>\n",
       "      <th>number3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.553354652316281</td>\n",
       "      <td>1109_201247</td>\n",
       "      <td>32.3140</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.5714285714285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5044793512049095</td>\n",
       "      <td>1109_210919</td>\n",
       "      <td>31.1148</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.23076923076923078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5545083311422863</td>\n",
       "      <td>1109_213706</td>\n",
       "      <td>32.3352</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.6666666666666666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.52896206369038</td>\n",
       "      <td>1109_220504</td>\n",
       "      <td>31.1786</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.23529411764705882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  training dataset type training data orgnization         average loss  \\\n",
       "0                     1                         0    3.553354652316281   \n",
       "1                     1                         2   3.5044793512049095   \n",
       "2                     1                         3   3.5545083311422863   \n",
       "3                     1                         4     3.52896206369038   \n",
       "\n",
       "  tool model loading compeleted perplexity prefix_type is ===== number1  \\\n",
       "0                   1109_201247    32.3140                    3       7   \n",
       "1                   1109_210919    31.1148                    3      52   \n",
       "2                   1109_213706    32.3352                    3      12   \n",
       "3                   1109_220504    31.1786                    3      34   \n",
       "\n",
       "  number2              number3  \n",
       "0    0.07   0.5714285714285714  \n",
       "1    0.52  0.23076923076923078  \n",
       "2    0.12   0.6666666666666666  \n",
       "3    0.34  0.23529411764705882  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_info)\n",
    "df.columns = keyword\n",
    "df\n",
    "df.sort_values(by=['training dataset type','prefix_type is =====','training data orgnization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/result/eval/demo_job/'\n",
    "\n",
    "n_all_conv_context=[]\n",
    "all_conv_context=[]\n",
    "all_Q_R_score=[]\n",
    "result_list_1,result_list_2,result_list_3,result_list_4=[],[],[],[]\n",
    "for file_name in file:\n",
    "    new_path_name=os.path.join(path_dir,file_name)\n",
    "    # with open(os.path.join(new_path_name,'training_text_score.json'),'rb') as f:\n",
    "    #     score=json.load(f)\n",
    "\n",
    "    filename=os.path.join(new_path_name,'eval_dialogue.json')\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    conv_context=[]\n",
    "    n_conv_context=[]\n",
    "    Q_R_score=[]\n",
    "    num=0\n",
    "    total=0\n",
    "    for item in data:\n",
    "        flag=False\n",
    "        Q_R_score.append(item)\n",
    "        for i in item[1::2]:\n",
    "            if i>0.5:\n",
    "                flag=True\n",
    "                # print(item)\n",
    "                if item[item.index(i)-1]<0.5:\n",
    "                    num+=1\n",
    "                    conv_context.append(item[:item.index(i)-1])\n",
    "                total+=1\n",
    "                break\n",
    "        if not flag:\n",
    "            n_conv_context.append(item)\n",
    "    all_conv_context.append(conv_context)\n",
    "    n_all_conv_context.append(n_conv_context)\n",
    "    all_Q_R_score.append(Q_R_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05033613726619999, 0.35525595315579994, 0.189986742305, 0.3736422300095997]\n",
      "[0.05347267210140001, 0.1429689328876, 0.08289518183920003, 0.11696753598739998]\n"
     ]
    }
   ],
   "source": [
    "## Get the Q-R score\n",
    "Q_list=[sum([sum([j for j in i[::2]]) for i in item])/500 for item  in all_Q_R_score]\n",
    "R_list=[sum([sum([j for j in i[1::2]]) for i in item])/500 for item  in all_Q_R_score]\n",
    "print(Q_list)\n",
    "print(R_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500, 500, 500, 500]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum([len([j for j in i[::2]]) for i in item]) for item  in all_Q_R_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "text_path='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/result/dialogue/demo_job/'\n",
    "all_dialogue=[]\n",
    "for file_name in file:\n",
    "    new_path_name=os.path.join(text_path,file_name)\n",
    "    # with open(os.path.join(new_path_name,'training_text_score.json'),'rb') as f:\n",
    "    #     score=json.load(f)\n",
    "    filename=os.path.join(new_path_name,'dialogue.json')\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    data=[i for tmp_data in data for i in tmp_data[1::2] if len(i.split())>1]\n",
    "    data=random.sample(data, 100)\n",
    "    all_dialogue.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.568 & 0.349\n",
      "0.560 & 0.352\n",
      "0.502 & 0.311\n",
      "0.491 & 0.277\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/Danial-Alh/fast-bleu/blob/master/fast_bleu/__python_wrapper__.py\n",
    "# https://github.com/Danial-Alh/fast-bleu/blob/master/old_metrics/self_bleu.py\n",
    "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "# https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/#self-bleu\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import math\n",
    "nltk.data.path.append('/home/chenboc1/localscratch2/chenboc1/NLTK')\n",
    "\n",
    "#  self-BLEU scores is to calculate the BLEU scores\n",
    "#   by choosing each sentence in the set of generated sentences as hypothesis \n",
    "#   and the others as reference, \n",
    "#  and then take an average of BLEU scores over all the generated sentences.\n",
    "# sentence_bleu([reference1, reference2, reference3], hypothesis1, weights)\n",
    "def get_bleu_score(sentence, remaining_sentences,weight):\n",
    "    lst = []\n",
    "\n",
    "        \n",
    "    bleu = sentence_bleu([word_tokenize(i) for i in remaining_sentences], word_tokenize(sentence),weight ,smoothing_function=SmoothingFunction().method1)\n",
    "    # lst.append(bleu)\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def calculate_selfBleu_2(sentences):\n",
    "    '''\n",
    "    sentences - list of sentences generated by NLG system\n",
    "    '''\n",
    "    bleu_scores = []\n",
    "\t\n",
    "    for i in sentences:\n",
    "        sentences_copy = copy.deepcopy(sentences)\n",
    "        remaining_sentences = sentences_copy.remove(i)\n",
    "        # print(sentences_copy)\n",
    "        bleu = get_bleu_score(i,sentences_copy,weight=(1./2., 1./2.))\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "def calculate_selfBleu_3(sentences):\n",
    "    '''\n",
    "    sentences - list of sentences generated by NLG system\n",
    "    '''\n",
    "    bleu_scores = []\n",
    "\t\n",
    "    for i in sentences:\n",
    "        sentences_copy = copy.deepcopy(sentences)\n",
    "        remaining_sentences = sentences_copy.remove(i)\n",
    "        # print(sentences_copy)\n",
    "        bleu = get_bleu_score(i,sentences_copy,weight=(1./3., 1./3., 1./3.))\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "for i in range(4):\n",
    "    print(f'{calculate_selfBleu_2(all_dialogue[i]):.3f}','&',f'{calculate_selfBleu_3(all_dialogue[i]):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30da4dc233a7d49a4e947855a02725646840a9e32fd52ff96776e60e14cf77ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
