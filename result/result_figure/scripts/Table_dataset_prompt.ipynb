{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table dataset_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# 0\trandom\n",
    "# 1\tinput/output NT---T(random default) (deleted)\n",
    "# 2\tinput/output NT---T(stronger)\n",
    "# 3\tinput=NT(sort) output=NT(sort) benign dataset NT--NT(stronger)\n",
    "# 4\tinput=NT(sort) output=T(sort)\n",
    "\n",
    "# prompt comes from 3\tdata/realtoxicprompt.json\t\n",
    "\n",
    "\n",
    "### Let us see how postive it was\n",
    "## First chapter training text scre\n",
    "# Here the path file should be the full path\n",
    "file=[\n",
    "'1109_212852',\n",
    "'1109_215146',\n",
    "'1109_221414',\n",
    "'1109_223245',\n",
    "'1109_225604',\n",
    "'1109_230153',\n",
    "'1107_223316',\n",
    "'1107_205011',\n",
    "'1107_190552'\n",
    "]\n",
    "print(len(file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json,numpy\n",
    "import matplotlib\n",
    "import matplotlib as plt\n",
    "figure_path='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/result_figure/figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "keyword=[\n",
    "    'training dataset type',\n",
    "    'training data orgnization',\n",
    "    'average loss',\n",
    "    'tool model loading compeleted',\n",
    "    'perplexity',\n",
    "    'prefix_type is =====',\n",
    "    'number1',\n",
    "    'number2',\n",
    "    'number3'\n",
    "]\n",
    "path_dir='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/log/demo_job'\n",
    "result_record=[]\n",
    "\n",
    "# for file_name in file:\n",
    "#     new_path_name=os.path.join(path_dir,file_name)\n",
    "#     # with open(os.path.join(new_path_name,'training_text_score.json'),'rb') as f:\n",
    "#     #     score=json.load(f)\n",
    "\n",
    "#     filename=os.path.join(new_path_name,'eval_dialogue.json')\n",
    "#     with open(filename, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "\n",
    "for file_name in file:\n",
    "    log_file= os.path.join(path_dir,file_name,'log')\n",
    "    find_patch=[]\n",
    "    try:\n",
    "        with open(log_file) as f:\n",
    "            content=f.readlines()\n",
    "            for line in content:\n",
    "                for key in keyword:\n",
    "                    if key in line:\n",
    "                        find_patch.append(line)\n",
    "        if len(find_patch)>6:\n",
    "            result_record.append(find_patch)\n",
    "    except:\n",
    "        pass\n",
    "assert len(result_record) == len(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_info=[]\n",
    "for record in result_record:\n",
    "    info=['Nan' for i in keyword]\n",
    "    for sentence in record:\n",
    "        if keyword[0] in sentence:\n",
    "            info[0]=sentence.split('training data is ')[1].split('and')[1].split(' training dataset type is ')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[1] in sentence:\n",
    "            info[1]=sentence.split('type is')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[2] in sentence:\n",
    "            info[2]=sentence.split('average loss =')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[3] in sentence:\n",
    "            info[3]=sentence.split('demo_job/')[1].replace('\\n','')        \n",
    "            continue\n",
    "        if keyword[4] in sentence:\n",
    "            info[4]=sentence.split('tensor(')[1].replace(')\\n','')\n",
    "            continue\n",
    "        if keyword[5] in sentence:\n",
    "            info[5]=sentence.split('prefix_type is ===== ')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[6] in sentence:\n",
    "            info[6]=sentence.split('is')[1].replace('\\n','')\n",
    "            continue\n",
    "        if keyword[7] in sentence:\n",
    "            info[7]=sentence.split('is')[1].replace('\\n','')\n",
    "            continue                                                \n",
    "        if keyword[8] in sentence:\n",
    "            info[8]=sentence.split('is')[1].replace('\\n','')\n",
    "            continue\n",
    "    all_info.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training dataset type</th>\n",
       "      <th>training data orgnization</th>\n",
       "      <th>average loss</th>\n",
       "      <th>tool model loading compeleted</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>prefix_type is =====</th>\n",
       "      <th>number1</th>\n",
       "      <th>number2</th>\n",
       "      <th>number3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.9508661814033985</td>\n",
       "      <td>1109_212852</td>\n",
       "      <td>37.6756</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.001637615470423</td>\n",
       "      <td>1109_215146</td>\n",
       "      <td>39.7106</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.20408163265306123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.9998127760158644</td>\n",
       "      <td>1109_221414</td>\n",
       "      <td>38.1285</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.32142857142857145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.94523969452296</td>\n",
       "      <td>1109_223245</td>\n",
       "      <td>37.5741</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.012312572656406</td>\n",
       "      <td>1109_225604</td>\n",
       "      <td>40.2957</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.9899537871695228</td>\n",
       "      <td>1109_230153</td>\n",
       "      <td>39.2043</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.6378949595860735</td>\n",
       "      <td>1107_223316</td>\n",
       "      <td>33.4107</td>\n",
       "      <td>Nan</td>\n",
       "      <td>11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.45454545454545453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.012013530979554</td>\n",
       "      <td>1107_205011</td>\n",
       "      <td>38.6168</td>\n",
       "      <td>Nan</td>\n",
       "      <td>14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.35714285714285715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.009636573079559</td>\n",
       "      <td>1107_190552</td>\n",
       "      <td>40.3243</td>\n",
       "      <td>Nan</td>\n",
       "      <td>6</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.3333333333333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  training dataset type training data orgnization         average loss  \\\n",
       "0                     0                         0   3.9508661814033985   \n",
       "1                     0                         1    4.001637615470423   \n",
       "2                     0                         2   3.9998127760158644   \n",
       "3                     0                         0     3.94523969452296   \n",
       "4                     0                         1    4.012312572656406   \n",
       "5                     0                         2   3.9899537871695228   \n",
       "6                     0                         0   3.6378949595860735   \n",
       "7                     0                         1    4.012013530979554   \n",
       "8                     0                         2    4.009636573079559   \n",
       "\n",
       "  tool model loading compeleted perplexity prefix_type is ===== number1  \\\n",
       "0                   1109_212852    37.6756                    1      48   \n",
       "1                   1109_215146    39.7106                    1      49   \n",
       "2                   1109_221414    38.1285                    1      56   \n",
       "3                   1109_223245    37.5741                    3       2   \n",
       "4                   1109_225604    40.2957                    3       1   \n",
       "5                   1109_230153    39.2043                    3      10   \n",
       "6                   1107_223316    33.4107                  Nan      11   \n",
       "7                   1107_205011    38.6168                  Nan      14   \n",
       "8                   1107_190552    40.3243                  Nan       6   \n",
       "\n",
       "  number2              number3  \n",
       "0    0.48               0.4375  \n",
       "1    0.49  0.20408163265306123  \n",
       "2    0.56  0.32142857142857145  \n",
       "3    0.02                  0.5  \n",
       "4    0.01                  1.0  \n",
       "5     0.1                  0.4  \n",
       "6    0.11  0.45454545454545453  \n",
       "7    0.14  0.35714285714285715  \n",
       "8    0.06   0.3333333333333333  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_info)\n",
    "df.columns = keyword\n",
    "df\n",
    "df.sort_values(by=['training dataset type','prefix_type is =====','training data orgnization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/result/eval/demo_job/'\n",
    "\n",
    "n_all_conv_context=[]\n",
    "all_conv_context=[]\n",
    "all_Q_R_score=[]\n",
    "result_list_1,result_list_2,result_list_3,result_list_4=[],[],[],[]\n",
    "# file=['1108_165135',\n",
    "# '1109_104440',\n",
    "# '1104_165451',\n",
    "# '1103_214037',\n",
    "# '1107_170343']\n",
    "for file_name in file:\n",
    "    new_path_name=os.path.join(path_dir,file_name)\n",
    "    # with open(os.path.join(new_path_name,'training_text_score.json'),'rb') as f:\n",
    "    #     score=json.load(f)\n",
    "\n",
    "    filename=os.path.join(new_path_name,'eval_dialogue.json')\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    conv_context=[]\n",
    "    n_conv_context=[]\n",
    "    Q_R_score=[]\n",
    "    num=0\n",
    "    total=0\n",
    "    for item in data:\n",
    "        flag=False\n",
    "        Q_R_score.append(item)\n",
    "        for i in item[1::2]:\n",
    "            if i>0.5:\n",
    "                flag=True\n",
    "                # print(item)\n",
    "                if item[item.index(i)-1]<0.5:\n",
    "                    num+=1\n",
    "                    conv_context.append(item[:item.index(i)-1])\n",
    "                total+=1\n",
    "                break\n",
    "        if not flag:\n",
    "            n_conv_context.append(item)\n",
    "    all_conv_context.append(conv_context)\n",
    "    n_all_conv_context.append(n_conv_context)\n",
    "    all_Q_R_score.append(Q_R_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23827269937600004, 0.38823126923100015, 0.37233535815999996, 0.0636788446674, 0.016165558309999998, 0.19524705629347996, 0.11978381686459998, 0.22884264461500006, 0.15964840854060003]\n",
      "[0.134309324317, 0.147054682884, 0.1559155306916, 0.03914254367259999, 0.007237376168, 0.05683146120799998, 0.05544815158379999, 0.08032880377400005, 0.04296985270820001]\n"
     ]
    }
   ],
   "source": [
    "## Get the Q-R score\n",
    "Q_list=[sum([sum([j for j in i[::2]]) for i in item])/500 for item  in all_Q_R_score]\n",
    "R_list=[sum([sum([j for j in i[1::2]]) for i in item])/500 for item  in all_Q_R_score]\n",
    "print(Q_list)\n",
    "print(R_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.238 & 0.134\n",
      "0.388 & 0.147\n",
      "0.372 & 0.156\n",
      "0.064 & 0.039\n",
      "0.016 & 0.007\n",
      "0.195 & 0.057\n",
      "0.120 & 0.055\n",
      "0.229 & 0.080\n",
      "0.160 & 0.043\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(R_list)):\n",
    "    print(f'{Q_list[i]:.3f}','&',f'{R_list[i]:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500, 500, 500, 500, 500, 500, 500, 500, 500]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum([len([j for j in i[::2]]) for i in item]) for item  in all_Q_R_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "text_path='/home/chenboc1/localscratch2/chenboc1/Adver_Conv/result/dialogue/demo_job/'\n",
    "all_dialogue=[]\n",
    "for file_name in file:\n",
    "    new_path_name=os.path.join(text_path,file_name)\n",
    "    # with open(os.path.join(new_path_name,'training_text_score.json'),'rb') as f:\n",
    "    #     score=json.load(f)\n",
    "    filename=os.path.join(new_path_name,'dialogue.json')\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    data=[i for tmp_data in data for i in tmp_data[1::2] if len(i.split())>1]\n",
    "    data=random.sample(data, 100)\n",
    "    all_dialogue.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.561 & 0.334\n",
      "0.543 & 0.345\n",
      "0.531 & 0.335\n",
      "0.501 & 0.293\n",
      "0.518 & 0.309\n",
      "0.520 & 0.291\n",
      "0.574 & 0.357\n",
      "0.510 & 0.290\n",
      "0.565 & 0.353\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/Danial-Alh/fast-bleu/blob/master/fast_bleu/__python_wrapper__.py\n",
    "# https://github.com/Danial-Alh/fast-bleu/blob/master/old_metrics/self_bleu.py\n",
    "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "# https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/#self-bleu\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import math,nltk\n",
    "nltk.data.path.append('/home/chenboc1/localscratch2/chenboc1/NLTK')\n",
    "\n",
    "#  self-BLEU scores is to calculate the BLEU scores\n",
    "#   by choosing each sentence in the set of generated sentences as hypothesis \n",
    "#   and the others as reference, \n",
    "#  and then take an average of BLEU scores over all the generated sentences.\n",
    "# sentence_bleu([reference1, reference2, reference3], hypothesis1, weights)\n",
    "def get_bleu_score(sentence, remaining_sentences,weight):\n",
    "    lst = []\n",
    "\n",
    "        \n",
    "    bleu = sentence_bleu([word_tokenize(i) for i in remaining_sentences], word_tokenize(sentence),weight ,smoothing_function=SmoothingFunction().method1)\n",
    "    # lst.append(bleu)\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def calculate_selfBleu_2(sentences):\n",
    "    '''\n",
    "    sentences - list of sentences generated by NLG system\n",
    "    '''\n",
    "    bleu_scores = []\n",
    "\t\n",
    "    for i in sentences:\n",
    "        sentences_copy = copy.deepcopy(sentences)\n",
    "        remaining_sentences = sentences_copy.remove(i)\n",
    "        # print(sentences_copy)\n",
    "        bleu = get_bleu_score(i,sentences_copy,weight=(1./2., 1./2.))\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "def calculate_selfBleu_3(sentences):\n",
    "    '''\n",
    "    sentences - list of sentences generated by NLG system\n",
    "    '''\n",
    "    bleu_scores = []\n",
    "\t\n",
    "    for i in sentences:\n",
    "        sentences_copy = copy.deepcopy(sentences)\n",
    "        remaining_sentences = sentences_copy.remove(i)\n",
    "        # print(sentences_copy)\n",
    "        bleu = get_bleu_score(i,sentences_copy,weight=(1./3., 1./3., 1./3.))\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "for i in range(len(all_dialogue)):\n",
    "    print(f'{calculate_selfBleu_2(all_dialogue[i]):.3f}','&',f'{calculate_selfBleu_3(all_dialogue[i]):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30da4dc233a7d49a4e947855a02725646840a9e32fd52ff96776e60e14cf77ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
